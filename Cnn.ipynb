{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "on_colab = False\n",
    "if on_colab:\n",
    "    from google.colab import drive\n",
    "    !pip install tqdm\n",
    "    !pip install Biopython\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn\n",
    "#import torchvision.transforms.functional as TF\n",
    "#from torchvision.transforms import ToTensor\n",
    "#from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dataset(data.Dataset):\n",
    "    '''\n",
    "    Checks whether a given sequence is legal\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq : String\n",
    "        Raw unprocessed string from the fasta file\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Bool\n",
    "    '''\n",
    "    def __is_legal_seq__(self, seq):\n",
    "        len_val = not (len(seq) > self.max_seq_len)\n",
    "        cont_val = not(('X' in seq) or ('B' in seq) or ('Z' in seq) or ('J' in seq))\n",
    "        return len_val and cont_val\n",
    "\n",
    "    '''\n",
    "    Generates a dictionary given a string with all the elements that will be in the dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    acids : String\n",
    "        An \"alphabet\" to generate the dictionary from.\n",
    "        The last char will be used as a padding value\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    acid_dict : Dictionary of Tensors\n",
    "        A dictionary with the same length as the input string.\n",
    "        It is effectively a one-hot encoding of acids\n",
    "    '''\n",
    "    def __gen_acid_dict__(self, acids):\n",
    "        acid_dict = {}\n",
    "        int_acid_dict = {}\n",
    "        int_to_acid_dict={}\n",
    "        for i, elem in enumerate(acids):\n",
    "            temp = torch.zeros(len(acids))\n",
    "            temp[i] = 1\n",
    "            acid_dict[elem] = temp\n",
    "            int_acid_dict[temp] = i\n",
    "            int_to_acid_dict[i] = temp\n",
    "        return acid_dict, int_acid_dict, int_to_acid_dict\n",
    "\n",
    "    '''\n",
    "    Initialisation for the Dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : String\n",
    "        Path to a fasta file with data.\n",
    "    max_seq_len : Int\n",
    "        An integer representing the longest sequences we want to take into account.\n",
    "    acids : String\n",
    "        An \"alphabet\" to generate the dictionary from.\n",
    "\n",
    "    Variables\n",
    "    ----------\n",
    "    acid_dict : Dictionary of Tensors\n",
    "        See __gen_acid_dict__\n",
    "    data : List of Strings\n",
    "        The entire input file loaded as strings\n",
    "    '''\n",
    "    def __init__(self, filename, max_seq_len, output_type=\"onehot\", acids=\"ACDEFGHIKLMNPQRSTVWY-\", get_prot_class=False):\n",
    "        elem_list = []\n",
    "        label_list = []\n",
    "        self.acids = acids\n",
    "        self.get_prot_class = get_prot_class\n",
    "        self.output_type = output_type\n",
    "        self.acid_dict, self.int_acid_dict, self.int_to_acid_dict = self.__gen_acid_dict__(acids)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # Loading the entire input file into memory\n",
    "        prot_class_re = re.compile(r\" (\\w)\\.\\d+\")\n",
    "        for i, elem in enumerate(SeqIO.parse(filename, \"fasta\")):\n",
    "            if self.__is_legal_seq__(elem.seq.upper()):\n",
    "                elem_list.append(elem.seq.upper())\n",
    "                if get_prot_class:\n",
    "                    label_list.append(prot_class_re.search(elem.description).group(1))\n",
    "        self.data = elem_list\n",
    "        self.prot_labels = label_list\n",
    "\n",
    "    '''\n",
    "    Method to get the length of the dataset\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    Int : Length of the entire dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    '''\n",
    "    Preprocesses a sequence into something usable by an LSTM and outputs it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    index : Int\n",
    "        Index to take the data from\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    tensor_seq : Tensor of size max_seq_len x len(acid_dict)\n",
    "        The padded, preprocessed Tensor of one-hot encoded acids.\n",
    "        If output_type=\"embed\" then it will have size max_seq_len\n",
    "    labels_seq : Tensor of size max_seq_len\n",
    "        Contains the labels for each element in tensor_seq\n",
    "        as the correct index of the one-hot encoding\n",
    "    valid_elems : Int\n",
    "        Integer value representing the length of the sequence before padding\n",
    "    '''\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.data[index]\n",
    "        #print(seq)\n",
    "        #print(self.acid_dict.keys())\n",
    "        valid_elems = min(len(seq), self.max_seq_len)\n",
    "\n",
    "        seq = str(seq).ljust(self.max_seq_len+1, self.acids[-1])\n",
    "        temp_seq = [self.acid_dict[x] for x in seq]\n",
    "        if self.output_type == \"embed\":\n",
    "            tensor_seq = torch.argmax(torch.stack(temp_seq[:-1]), dim=1).long()\n",
    "        else:\n",
    "            tensor_seq = torch.stack(temp_seq[:-1], dim=0).float()#.view(self.max_seq_len, 1, -1)\n",
    "\n",
    "        # Labels consisting of the index of correct class\n",
    "        #                                               |\n",
    "        #                                   CHANGE THIS V TO 1: WHEN FINISHED PREDICTING IDENTITY\n",
    "        labels_seq = torch.argmax(torch.stack(temp_seq[1:]), dim=1).long()#.view(-1, 1)\n",
    "        if self.get_prot_class:\n",
    "            return tensor_seq, labels_seq, valid_elems, self.prot_labels[index]\n",
    "        else:\n",
    "            return tensor_seq, labels_seq, valid_elems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(len(gc.get_objects()))\n",
    "acids = \"ACDEFGHIKLMNOPQRSTUVWY-\"\n",
    "\n",
    "large_file = \"uniref50.fasta\"\n",
    "\n",
    "if on_colab:\n",
    "    small_file = \"/content/gdrive/My Drive/proteinData/100k_rows.fasta\"\n",
    "\n",
    "    small_label_file1 = \"/content/gdrive/My Drive/proteinData/astral-scopedom-seqres-gd-sel-gs-bib-40-2.07.fasta\"\n",
    "    big_label_file1 = \"/content/gdrive/My Drive/proteinData/astral-scopedom-seqres-gd-sel-gs-bib-95-2.07.fasta\"\n",
    "\n",
    "    small_label_file2 = \"/content/gdrive/My Drive/proteinData/scope_data_40.fasta\"\n",
    "    big_label_file2 = \"/content/gdrive/My Drive/proteinData/scope_data_95.fasta\"\n",
    "else:\n",
    "    large_file = \"uniref50.fasta\"\n",
    "    small_file = \"100k_rows.fasta\"\n",
    "    small_label_file2 = \"scope_data_40.fasta\"\n",
    "    big_label_file2 = \"scope_data_95.fasta\"\n",
    "\n",
    "test_file = \"test.fasta\"\n",
    "\n",
    "max_seq_len = 500\n",
    "\n",
    "# Good sizes: 16/700 or 32/400 on laptop\n",
    "# 32/1500 on desktop\n",
    "\n",
    "#hidden_dim = 200\n",
    "\n",
    "#hidden_layers = 1\n",
    "\n",
    "# Use Cuda if available\n",
    "use_cuda = torch.cuda.is_available() and True\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "batch_size = 64\n",
    "dataset = Dataset(small_file, max_seq_len, acids=acids)\n",
    "base_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_space):\n",
    "        super(CNN, self).__init__()\n",
    "        self.latent_dim = latent_space\n",
    "\n",
    "        self.embed = nn.Embedding(23, 30)\n",
    "\n",
    "        #Encode Layer\n",
    "        self.conv1 = nn.Conv1d(30, 20, 5, padding=1)#self.conv(30, 15, 5)\n",
    "        self.conv2 = nn.Conv1d(20, 14, 5, padding=1)#self.conv(15, 8, 5)\n",
    "        self.conv3 = nn.Conv1d(14, 8, 5, padding=1)#self.conv(8, 4, 5)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(8, 4, 5, padding=1)\n",
    "\n",
    "        self.conv_mid = nn.Conv1d(4,4,5,padding=2)\n",
    "    \n",
    "        #Decode Layer\n",
    "        self.conv5 = nn.Conv1d(4, 8, 5, padding=1)#self.conv(4, 8, 5)\n",
    "        self.conv6 = nn.Conv1d(8, 14, 5, padding=1)#self.conv(8, 14, 5)\n",
    "        self.conv7 = nn.Conv1d(14, 20, 5, padding=1)#self.conv(14, 20, 5)\n",
    "        self.conv8 = nn.Conv1d(20, 23, 5, padding=2)#self.conv(20, 23, 5)\n",
    "\n",
    "        #self.Max_pool = torch.nn.MaxPool1d(2,return_indices=True)\n",
    "        self.Avg_pool = torch.nn.AvgPool1d(2)\n",
    "\n",
    "        self.Latent_avg_pool =  nn.AdaptiveAvgPool1d(self.latent_dim)#nn.AdaptiveMaxPool1d(self.latent_dim)\n",
    "\n",
    "        self.Up_sample_first = nn.Upsample(62, scale_factor=None, align_corners=None)\n",
    "        self.Up_sample_mid = nn.Upsample(size=None, scale_factor=2, align_corners=None)\n",
    "        self.Up_sample_last = nn.Upsample(size=500, scale_factor=None, align_corners=None)\n",
    "\n",
    "        #self.UnPool = nn.MaxUnpool1d(2, stride=2)\n",
    "    \n",
    "    def initialize(self, input_data):\n",
    "        init_x = self.embed(input_data)\n",
    "        init_x = torch.transpose(init_x, 1, 2)\n",
    "        #init_x = rnn.pack_padded_sequence(init_x, valid_elems, enforce_sorted=False, batch_first=True)\n",
    "        return init_x\n",
    "\n",
    "    def Encode(self,data):\n",
    "        x = F.relu(self.conv1(data))\n",
    "        x = self.Avg_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.Avg_pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.Avg_pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.Latent_avg_pool(x)\n",
    "        x = self.conv_mid(x)\n",
    "        return x\n",
    "  \n",
    "    def Decode(self,x):\n",
    "\n",
    "        x_con = self.Up_sample_first(x)\n",
    "        x_con = F.relu(self.conv5(x_con))\n",
    "\n",
    "        x_con = self.Up_sample_mid(x_con)\n",
    "        x_con = F.relu(self.conv6(x_con))\n",
    "\n",
    "        x_con = self.Up_sample_mid(x_con)\n",
    "        x_con = F.relu(self.conv7(x_con))\n",
    "\n",
    "        x_con = self.Up_sample_last(x_con)\n",
    "        x_con = F.relu(self.conv8(x_con))\n",
    "\n",
    "        return x_con\n",
    "\n",
    "    def forward(self, data):\n",
    "        init_data = self.initialize(data)\n",
    "        x = self.Encode(init_data)\n",
    "        x_con = self.Decode(x)\n",
    "        #x_con = torch.sigmoid(x_con)\n",
    "        return x_con, torch.flatten(x, start_dim=1)\n",
    "\n",
    "    def save(self, filename):\n",
    "        args_dict = {\n",
    "            \"latent_space\": self.latent_dim,\n",
    "        }\n",
    "        torch.save({\n",
    "            \"state_dict\": self.state_dict(),\n",
    "            \"args_dict\": args_dict\n",
    "        }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset = Dataset(small_file, max_seq_len, output_type=\"embed\", acids=acids)\n",
    "base_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "\n",
    "model = CNN(50).to(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-6\n",
    "loss_func = nn.CrossEntropyLoss(reduction=\"mean\").to(processor)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)#optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30)\n",
    "\n",
    "loss_list = []\n",
    "epochs = 100\n",
    "time_diff=0\n",
    "\n",
    "batches = float(\"inf\")\n",
    "min_loss = float(\"inf\")\n",
    "no_improv = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for batch_index, (batch, labels, valid_elems) in enumerate(base_generator):\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        est_time_left = str(time_diff*(min(batches, dataset.__len__()/batch_size) - batch_index) + (time_diff*min(batches, dataset.__len__()/batch_size)) * (epochs - (epoch+1))).split(\".\")[0]\n",
    "        \n",
    "        sys.stdout.write(\"\\rEpoch: {0}. Batch: {1}. Min loss: {2:.5f}. Estimated time left: {3}. Best: {4} batches ago.\".format(epoch+1, batch_index+1, min_loss, est_time_left, no_improv))\n",
    "        #optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(processor)\n",
    "        \n",
    "        #labels = torch.argmax(batch,dim=2)\n",
    "        \n",
    "        #batch = torch.transpose(batch, 1,2)\n",
    "        outputs, x = model(batch)\n",
    "\n",
    "        outputs = torch.transpose(outputs, 1, 2)\n",
    "\n",
    "        outputs = rnn.pack_padded_sequence(outputs, valid_elems, enforce_sorted=False, batch_first=True)\n",
    "        labels = rnn.pack_padded_sequence(batch, valid_elems, enforce_sorted=False, batch_first=True)\n",
    "\n",
    "        #labels = rnn.pack_padded_sequence(labels, valid_elems, enforce_sorted=False, batch_first=True)\n",
    "\n",
    "        loss = loss_func(outputs.data, labels.data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if batch_index % 500==0:\n",
    "        #  print(\"epoch: {}. batch_index: {}. Loss = {}\".format(epoch, batch_index, loss))\n",
    "        \n",
    "        if loss.item() < min_loss:\n",
    "            no_improv = 0\n",
    "            model.save(\"temp_best_cnn_model.pth\")\n",
    "            min_loss = loss.item()\n",
    "        else:\n",
    "            no_improv += 1\n",
    "          \n",
    "        loss_list.append(loss.item())\n",
    "    #scheduler.step()\n",
    "    end_time = datetime.datetime.now()\n",
    "    time_diff = end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(loss_list))\n",
    "plt.plot(list(range(len(loss_list))), loss_list)\n",
    "plt.show()\n",
    "\n",
    "load_model = True\n",
    "\n",
    "if load_model:\n",
    "    loaded_params = torch.load(\"temp_best_cnn_model.pth\")\n",
    "    #model = CNN(**loaded_params[\"args_dict\"]).to(processor)\n",
    "    model = CNN(100).to(processor)\n",
    "    model.load_state_dict(loaded_params[\"state_dict\"])\n",
    "    \n",
    "batch_size1=1\n",
    "\n",
    "dataset = Dataset(small_file, max_seq_len, output_type=\"embed\", acids=acids)\n",
    "base_generator = data.DataLoader(dataset, batch_size=batch_size1, shuffle=True, num_workers=16)\n",
    "\n",
    "def print_seq(preds, valid, alphabet):\n",
    "    for i, seq in enumerate(preds):\n",
    "        print(\"Sequence {}\".format(i))\n",
    "        indexes = torch.argmax(seq[:valid[i]], dim=1)\n",
    "        ret_val = [alphabet[x] for x in indexes]\n",
    "        print(\"\".join(ret_val))\n",
    "        return ret_val\n",
    "\n",
    "for batch_index, (batch, labels, valid_elems) in enumerate(base_generator):\n",
    "      batch = batch.to(processor)\n",
    "\n",
    "      pred, x = model(batch)\n",
    "\n",
    "      pred = torch.squeeze(pred,dim=0)\n",
    "      batch = torch.squeeze(batch,dim=\n",
    "                            0)\n",
    "      batch = torch.stack([dataset.int_to_acid_dict[int(elem)] for elem in batch.cpu()])\n",
    "\n",
    "      batch = torch.transpose(batch, 0,1)\n",
    "\n",
    "      pred = pred.view(1,23,500)\n",
    "      batch = batch.view(1,23,500)\n",
    "\n",
    "      batch = torch.transpose(batch, 1,2)\n",
    "      pred = torch.transpose(pred, 1,2)\n",
    "\n",
    "      break\n",
    "\n",
    "print(\"\\nInput\")\n",
    "inp_str = print_seq(batch[0].view(1,batch.size()[1], batch.size()[2]), valid_elems, acids)\n",
    "\n",
    "print(\"\\nPrediction\")\n",
    "pred_str = print_seq(pred[0].view(1,pred.size()[1], pred.size()[2]), valid_elems, acids)\n",
    "\n",
    "acc = np.sum([1 if pred_elem == original_elem else 0 for pred_elem, original_elem in zip(inp_str,pred_str)])/len(pred_str)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def structure_to_int(structure_chars):\n",
    "    count = 0 \n",
    "    char_to_int = {}\n",
    "    int_to_char = {}\n",
    "    for label in structure_chars:\n",
    "      if label not in char_to_int:\n",
    "        char_to_int.update({label : count})\n",
    "        int_to_char.update({count : label})\n",
    "        count += 1\n",
    "    return char_to_int, int_to_char\n",
    "\n",
    "def plot_data(nr_plots, model_output, labels, colors, title, rev_structure_labels):\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    for unique in np.unique(labels):\n",
    "      mask = [elem==unique and unique != 'd' for elem in labels]\n",
    "      unique_list = model_output[mask]\n",
    "      ax.scatter(unique_list[:,0], unique_list[:,1], label=rev_structure_labels[unique], marker='.')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_points = []\n",
    "structure_labels = []\n",
    "batch_size1 = 1\n",
    "\n",
    "structure_chars = Chars = \"dcabgfe\"\n",
    "\n",
    "char_to_int_dict, rev_structure_labels = structure_to_int(structure_chars)\n",
    "\n",
    "\n",
    "dataset = Dataset(small_label_file2, max_seq_len, acids=acids, get_prot_class=True)\n",
    "base_generator = data.DataLoader(dataset, batch_size=batch_size1, shuffle=True, num_workers=16)\n",
    "\n",
    "for batch_index, (batch, labels, valid_elems, protein_label) in enumerate(base_generator):\n",
    "    if protein_label[0] == 'd':\n",
    "      continue\n",
    "    batch = torch.transpose(batch, 1,2).to(processor)\n",
    "    output, x_out = model(batch)\n",
    "\n",
    "    x_out = torch.squeeze(torch.squeeze(x_out, dim=0),dim=0)\n",
    "    data_points.append((x_out.cpu()).detach().numpy())\n",
    "    if protein_label[0] == 'd':\n",
    "      print(protein_label[0])\n",
    "\n",
    "    structure_labels.append(char_to_int_dict[protein_label[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_points = np.array(data_points)\n",
    "structure_labels = np.array(structure_labels)\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "PCA_data = pca.fit_transform(data_points)\n",
    "\n",
    "t_sne_data = TSNE(n_components=2, perplexity=15, learning_rate=200).fit_transform(data_points)\n",
    "\n",
    "\n",
    "#z = low_dim_points[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = ['Grey', 'Purple', 'Blue', 'Green', 'Orange', 'Red',\n",
    "          'Yellow', 'Black']\n",
    "        \n",
    "#pca = PCA(n_components = 2)\n",
    "#low_dim_points = pca.fit_transform(data_points)\n",
    "\n",
    "\n",
    "data_points = np.array(data_points)\n",
    "x = t_sne_data[:,0]\n",
    "y = t_sne_data[:,1]\n",
    "\n",
    "plt.scatter(x,y, marker= '.', s=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(loss_func)\n",
    "lossfun = \"CrossEntropy\"\n",
    "optimizer = \"Adagrad\"\n",
    "\n",
    "t_sne_title = \"T-sne, using batch_size: {}, lr: {}, epochs: {}, loss_func: {}, optimizer: {}\".format(batch_size, lr, epochs, lossfun,optimizer)\n",
    "PCA_title = \"PCA, using batch_size: {}, lr: {}, epochs: {}, loss_func: {}, optimizer: {}\".format(batch_size, lr, epochs, lossfun,optimizer)\n",
    "print(t_sne_title)\n",
    "print(PCA_title)\n",
    "\n",
    "plot_data(10000,t_sne_data,structure_labels, colors,t_sne_title,rev_structure_labels)\n",
    "plot_data(10000,PCA_data,structure_labels, colors,PCA_title,rev_structure_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Cnn.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
