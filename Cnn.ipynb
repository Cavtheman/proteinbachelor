{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "\n",
    "!pip install Biopython\n",
    "from Bio import SeqIO\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    # Checks whether a given sequence is legal\n",
    "    def __is_legal_seq__(self, seq):\n",
    "        len_val = not (len(seq) > self.max_seq_len)\n",
    "        cont_val = not(('X' in seq) or ('B' in seq) or ('Z' in seq) or ('J' in seq))\n",
    "        return len_val and cont_val\n",
    "\n",
    "    # Generates a dictionary given a string with all the elements\n",
    "    def __gen_acid_dict__(self, acids):\n",
    "        acid_dict = {}\n",
    "        int_acid_dict = {}\n",
    "        for i, elem in enumerate(acids):\n",
    "            temp = torch.zeros(len(acids))\n",
    "            temp[i] = 1\n",
    "            acid_dict[elem] = temp\n",
    "            int_acid_dict[temp] = i\n",
    "        return acid_dict, int_acid_dict\n",
    "\n",
    "    def __init__(self, filename, max_seq_len, acids=\"ACDEFGHIKLMNPQRSTVWY-\", int_version=False):\n",
    "        elem_list = []\n",
    "        self.acids = acids\n",
    "        self.acid_dict, self.int_acid_dict = self.__gen_acid_dict__(acids)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.int_version = int_version\n",
    "        # Loading the entire input file into memory\n",
    "        for i, elem in enumerate(SeqIO.parse(filename, \"fasta\")):\n",
    "            if self.__is_legal_seq__(elem.seq):\n",
    "                elem_list.append(elem.seq)\n",
    "        self.data = elem_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __prepare_seq__(self, seq):\n",
    "        valid_elems = min(len(seq)+1, self.max_seq_len)\n",
    "        seq = str(seq).ljust(self.max_seq_len+1, '-')\n",
    "        \n",
    "        temp_seq = [self.acid_dict[x] for x in seq]\n",
    "\n",
    "        tensor_seq = torch.stack(temp_seq[:-1]).float()\n",
    "    \n",
    "        labels_seq = torch.stack(temp_seq[1:]).float()\n",
    "        \n",
    "        if self.int_version:\n",
    "            temp_seq = [self.int_acid_dict[x]/len(acids) for x in temp_seq]\n",
    "            tensor_seq = torch.tensor(temp_seq[:-1]).float()\n",
    "            labels_seq = torch.tensor(temp_seq[1:])\n",
    "\n",
    "        return tensor_seq, labels_seq, valid_elems\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__prepare_seq__(self.data[index])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(gc.get_objects()))\n",
    "acids = \"ACDEFGHIKLMNOPQRSTUVWY-\"\n",
    "large_file = \"uniref50.fasta\"\n",
    "small_file = \"/content/gdrive/My Drive/proteinData/100k_rows.fasta\"\n",
    "test_file = \"test.fasta\"\n",
    "\n",
    "max_seq_len = 2000\n",
    "\n",
    "# Good sizes: 16/700 or 32/400 on laptop\n",
    "# 32/1500 on desktop\n",
    "batch_size = 32\n",
    "#hidden_dim = 200\n",
    "\n",
    "#hidden_layers = 1\n",
    "\n",
    "# Use Cuda if available\n",
    "use_cuda = torch.cuda.is_available() and True\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "dataset = Dataset(small_file, max_seq_len, acids=acids)\n",
    "base_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def conv(self,in_channels,out_channels, kernelSize):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels, kernelSize, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels,out_channels, kernelSize, padding=1),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def __init__(self, channel_size, kernel_size=3):\n",
    "        super(CNN, self).__init__()\n",
    "        self.in_channels = channel_size\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.pool0 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "        #self.pool2 = torch.nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.dropout_02 = nn.Dropout2d(p=0.2)\n",
    "        self.dropout_04 = nn.Dropout2d(p=0.4)\n",
    "\n",
    "        self.conv0 = self.conv(channel_size, 16, kernel_size)\n",
    "        self.conv1 = self.conv(16, 32, kernel_size)\n",
    "        self.conv2 = self.conv(32, 64, kernel_size)\n",
    "        #self.conv3 = self.conv(64,128, kernel_size)\n",
    "\n",
    "        #self.conv4 = self.conv(128, 64, kernel_size)\n",
    "        self.conv3 = self.conv(64, 32, kernel_size)\n",
    "        self.conv4 = self.conv(32,16, kernel_size)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(16,1, 1, padding=0)\n",
    "\n",
    "        self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.up_conv0 = self.conv(64,32,kernel_size)\n",
    "        self.up_conv1 = self.conv(32,16, kernel_size)\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        # first layer\n",
    "        layer0 = self.conv0(data)\n",
    "        print(\"layer0: \", layer0.size())\n",
    "\n",
    "        layer1 = self.pool0(layer0)\n",
    "        layer1 = self.conv1(layer1)\n",
    "\n",
    "        print(\"layer1: \", layer1.size())\n",
    "        #Second and last layer\n",
    "        layer2 = self.pool1(layer1)\n",
    "        layer2 = self.conv2(layer2)\n",
    "        print(\"layer2: \", layer2.size())\n",
    "\n",
    "        #first uplayer\n",
    "        up_layer2 = self.up_sample(layer2)\n",
    "        \n",
    "        up_layer2 = self.up_conv0(up_layer2)\n",
    "        print(\"up_layer2: \", up_layer2.size())\n",
    "        up_layer2 = torch.cat([up_layer2,layer1], dim=1)\n",
    "        print(\"up_layer2_cat: \", up_layer2.size())\n",
    "        up_layer2 = self.conv3(up_layer2)\n",
    "        print(\"up_layer2_cat: \", up_layer2.size())\n",
    "\n",
    "        #second and last uplayer\n",
    "        up_layer1 = self.up_sample(up_layer2)\n",
    "        up_layer1 = self.up_conv1(up_layer1)\n",
    "        up_layer1 = torch.cat([up_layer1, layer0], dim=1)\n",
    "        up_layer1 = self.conv4(up_layer1)\n",
    "\n",
    "        final_conv = self.final_conv(up_layer1)\n",
    "\n",
    "        final_conv = torch.sigmoid(final_conv)\n",
    "\n",
    "        return final_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CNN(1).cuda()\n",
    "\n",
    "loss_func = nn.MSELoss().cuda()\n",
    "optimizer = optim.Adamax(test.parameters(), lr=0.1)\n",
    "loss_list = []\n",
    "epochs = 50\n",
    "time_diff=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    sys.stdout.write(\"\\rCurrently at epoch: \" + str(epoch+1) + \". Estimated time remaining: {}\".format(time_diff*(epochs - epoch)))\n",
    "    running_loss = 0.0\n",
    "    for batch_index, (batch, labels, valid_elems) in enumerate(base_generator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = (batch.view(batch_size, 1, max_seq_len,len(acids))).cuda()\n",
    "        labels = (labels.view(batch_size,1,max_seq_len,len(acids))).cuda()\n",
    "        batch = F.pad(batch, (1,0,0,0))\n",
    "        labels = F.pad(labels, (1,0,0,0))\n",
    "        print(batch.size(),\"\\n\")\n",
    "        \n",
    "\n",
    "        outputs = test(batch)\n",
    "\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "    end_time = datetime.datetime.now()\n",
    "    time_diff = end_time - start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
