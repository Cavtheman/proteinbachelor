{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "\n",
    "!pip install tqdm\n",
    "import tqdm\n",
    "\n",
    "!pip install Biopython\n",
    "from Bio import SeqIO\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    # Checks whether a given sequence is legal\n",
    "    def __is_legal_seq__(self, seq):\n",
    "        len_val = not (len(seq) > self.max_seq_len)\n",
    "        cont_val = not(('X' in seq) or ('B' in seq) or ('Z' in seq) or ('J' in seq))\n",
    "        return len_val and cont_val\n",
    "\n",
    "    # Generates a dictionary given a string with all the elements\n",
    "    def __gen_acid_dict__(self, acids):\n",
    "        acid_dict = {}\n",
    "        int_acid_dict = {}\n",
    "        for i, elem in enumerate(acids):\n",
    "            temp = torch.zeros(len(acids))\n",
    "            temp[i] = 1\n",
    "            acid_dict[elem] = temp\n",
    "            int_acid_dict[temp] = i\n",
    "        return acid_dict, int_acid_dict\n",
    "\n",
    "    def __init__(self, filename, max_seq_len, acids=\"ACDEFGHIKLMNPQRSTVWY-\", int_version=False):\n",
    "        elem_list = []\n",
    "        self.acids = acids\n",
    "        self.acid_dict, self.int_acid_dict = self.__gen_acid_dict__(acids)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.int_version = int_version\n",
    "        # Loading the entire input file into memory\n",
    "        for i, elem in enumerate(SeqIO.parse(filename, \"fasta\")):\n",
    "            if self.__is_legal_seq__(elem.seq):\n",
    "                elem_list.append(elem.seq)\n",
    "        self.data = elem_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __prepare_seq__(self, seq):\n",
    "      valid_elems = min(len(seq)+1, self.max_seq_len)\n",
    "\n",
    "      seq = str(seq).ljust(self.max_seq_len+1, self.acids[-1])\n",
    "      temp_seq = [self.acid_dict[x] for x in seq]\n",
    "      tensor_seq = torch.stack(temp_seq[:-1], dim=0).float()#.view(self.max_seq_len, 1, -1)\n",
    "\n",
    "      # Labels consisting of the index of correct class\n",
    "      labels_seq = torch.argmax(torch.stack(temp_seq[1:]), dim=1).long()#.view(-1, 1)\n",
    "\n",
    "      return tensor_seq, labels_seq, valid_elems\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__prepare_seq__(self.data[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(gc.get_objects()))\n",
    "acids = \"ACDEFGHIKLMNOPQRSTUVWY-\"\n",
    "large_file = \"uniref50.fasta\"\n",
    "small_file = \"/content/gdrive/My Drive/proteinData/100k_rows.fasta\"\n",
    "small_label_file = \"/content/gdrive/My Drive/proteinData/astral-scopedom-seqres-gd-sel-gs-bib-40-2.07.fasta\"\n",
    "big_label_file = \"/content/gdrive/My Drive/proteinData/astral-scopedom-seqres-gd-sel-gs-bib-95-2.07.fasta\"\n",
    "test_file = \"test.fasta\"\n",
    "\n",
    "max_seq_len = 500\n",
    "\n",
    "# Good sizes: 16/700 or 32/400 on laptop\n",
    "# 32/1500 on desktop\n",
    "batch_size = 32\n",
    "#hidden_dim = 200\n",
    "\n",
    "#hidden_layers = 1\n",
    "\n",
    "# Use Cuda if available\n",
    "use_cuda = torch.cuda.is_available() and True\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "dataset = Dataset(small_file, max_seq_len, acids=acids, int_version=False)\n",
    "base_generator = data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, latent_space):\n",
    "    super(CNN, self).__init__()\n",
    "    self.latent_dim = latent_space\n",
    "    \n",
    "    #Encode Layer\n",
    "    self.conv1 = nn.Conv1d(23, 15, 3, padding=1)\n",
    "    self.conv2 = nn.Conv1d(15, 8, 3, padding=1)\n",
    "    self.conv3 = nn.Conv1d(8, 4, 3, padding=1)\n",
    "    self.conv4 = nn.Conv1d(4, 1, 3, padding=1)\n",
    "\n",
    "    #Decode layer\n",
    "    #self. t_conv1 = nn.ConvTranspose1d(184,92,3, padding=1)\n",
    "    #self. t_conv2 = nn.ConvTranspose1d(92,46,3, padding=1)\n",
    "    #self. t_conv3 = nn.ConvTranspose1d(46,23,3, padding=1)\n",
    "\n",
    "    self.conv5 = nn.Conv1d(1, 4, 3, padding=1)\n",
    "    self.conv6 = nn.Conv1d(4, 8, 3, padding=1)\n",
    "    self.conv7 = nn.Conv1d(8, 15, 3, padding=1)\n",
    "    self.conv8 = nn.Conv1d(15, 23, 3, padding=1)\n",
    "\n",
    "    self.Max_pool = torch.nn.MaxPool1d(2,return_indices=True)\n",
    "    self.Avg_pool = torch.nn.AvgPool1d(2)\n",
    "\n",
    "    self.Latent_avg_pool =  nn.AdaptiveAvgPool1d(self.latent_dim)\n",
    "\n",
    "    self.Up_sample_first = nn.Upsample(size=16, scale_factor=None, mode='nearest', align_corners=None)\n",
    "    self.Up_sample_mid = nn.Upsample(size=None, scale_factor=2, mode='nearest', align_corners=None)\n",
    "    self.Up_sample_last = nn.Upsample(size=500, scale_factor=None, mode='nearest', align_corners=None)\n",
    "\n",
    "    self.UnPool = nn.MaxUnpool1d(2, stride=2)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "  def Encode(self,data):\n",
    "    \"\"\"\n",
    "    x = F.relu(self.conv1(data))\n",
    "    x, id1 = self.Max_pool(x)\n",
    "\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x, id2 = self.Max_pool(x)\n",
    "\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x, id3 = self.Max_pool(x)\n",
    "\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x, id3 = self.Max_pool(x)\n",
    "    \"\"\"\n",
    "    x = F.relu(self.conv1(data))\n",
    "    x = self.Avg_pool(x)\n",
    "\n",
    "    x = F.relu(self.conv2(x))\n",
    "    x = self.Avg_pool(x)\n",
    "\n",
    "    x = F.relu(self.conv3(x))\n",
    "    x = self.Avg_pool(x)\n",
    "\n",
    "    x = F.relu(self.conv4(x))\n",
    "    x = self.Latent_avg_pool(x)\n",
    "\n",
    "    return x#, id1, id2, id3\n",
    "  \n",
    "  def Decode(self,x):#, id1, id2, id3):\n",
    "    \"\"\"\n",
    "    x_con = self.UnPool(x, id3, output_size=id2.size())\n",
    "    x_con = F.relu(self.conv4(x_con))\n",
    "\n",
    "    x_con = self.UnPool(x_con, id2)\n",
    "    x_con = F.relu(self.conv5(x_con))\n",
    "\n",
    "    x_con = self.UnPool(x_con, id1)\n",
    "    x_con = F.relu(self.conv6(x_con))\n",
    "    \"\"\"\n",
    "    #print(\"lol\", x.size())\n",
    "    x_con = F.relu(self.conv5(x))\n",
    "    x_con = self.Up_sample_first(x_con)\n",
    "    x_con = self.Up_sample_mid(x_con)\n",
    "    #print(\"lol\", x_con.size())\n",
    "\n",
    "    x_con = F.relu(self.conv6(x_con))\n",
    "    x_con = self.Up_sample_mid(x_con)\n",
    "    #print(x_con.size())\n",
    "    \n",
    "    x_con = F.relu(self.conv7(x_con))\n",
    "    x_con = self.Up_sample_mid(x_con)\n",
    "    #print(x_con.size())\n",
    "    \n",
    "    #x_con = F.relu(self.conv8(x_con))\n",
    "    x_con = self.conv8(x_con)\n",
    "    x_con = self.Up_sample_last(x_con)\n",
    "    #print(x_con.size())\n",
    "    \n",
    "    return x_con\n",
    "\n",
    "\n",
    "  def forward(self, data):\n",
    "    #print(\"starting Encoding\")\n",
    "    x = self.Encode(data)\n",
    "    #print(\"starting Decoding\")\n",
    "    #print(\"id1: {}\\nid2: {}\\nid3: {}\".format(id1.size(), id2.size(), id3.size()))\n",
    "    x_con = self.Decode(x)#, id1, id2, id3)\n",
    "    return x_con,x\n",
    "\n",
    "test = CNN(3)\n",
    "lel = torch.zeros(32,23,500).float()\n",
    "juhu = test(lel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CNN(2).cuda()\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss().cuda()#nn.NLLLoss()#nn.BCELoss().cuda()\n",
    "optimizer = optim.Adam(test.parameters(), lr=0.00005)#optim.SGD(test.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_list = []\n",
    "epochs = 10\n",
    "time_diff=0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = datetime.datetime.now()\n",
    "\n",
    "    sys.stdout.write(\"\\rCurrently at epoch: \" + str(epoch+1) + \". Estimated time remaining: {}\\n\".format(time_diff*(epochs - epoch)))\n",
    "    running_loss = 0.0\n",
    "    for batch_index, (batch, labels, valid_elems) in enumerate(base_generator):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.cuda()\n",
    "        labels = labels.cuda()\n",
    "        labels = torch.argmax(batch,dim=2)\n",
    "        \n",
    "        batch = torch.transpose(batch, 1,2)\n",
    "        \n",
    "        outputs, x = test(batch.data)\n",
    "\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_index % 500==0:\n",
    "          print(\"epoch: {}. batch_index: {}. Loss = {}\".format(epoch, batch_index, loss))\n",
    "          \n",
    "        loss_list.append(loss.item())\n",
    "    end_time = datetime.datetime.now()\n",
    "    time_diff = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loss_list))\n",
    "plt.plot(list(range(len(loss_list))), loss_list)\n",
    "plt.show()\n",
    "\n",
    "def print_seq(preds, valid, alphabet):\n",
    "    for i, seq in enumerate(preds):\n",
    "        print(\"Sequence {}\".format(i))\n",
    "        indexes = torch.argmax(seq[:valid[i]], dim=1)\n",
    "        ret_val = [alphabet[x] for x in indexes]\n",
    "        print(\"\".join(ret_val))\n",
    "        return ret_val\n",
    "\n",
    "mean = 0\n",
    "count = 0\n",
    "for i, seq in enumerate(SeqIO.parse(small_file, \"fasta\")):\n",
    "  if (len(seq.seq) > max_seq_len) or (('X' in seq.seq) or ('B' in seq.seq) or ('Z' in seq.seq) or ('J' in seq.seq)):\n",
    "    continue\n",
    "  original = str(seq.seq).ljust(max_seq_len, '-')\n",
    "  seq_tensor = str(seq.seq[:-1]).ljust(max_seq_len, '-')\n",
    "  label_tensor = str(seq.seq[1:]).ljust(max_seq_len, '-')\n",
    "  print(seq_tensor)\n",
    "\n",
    "  original = [dataset.acid_dict[seq_elem] for seq_elem in original]\n",
    "  seq_tensor = [dataset.acid_dict[seq_elem] for seq_elem in seq_tensor]\n",
    "  label_tensor = [dataset.acid_dict[seq_elem] for seq_elem in label_tensor]\n",
    "\n",
    "  original = torch.stack(original).float()\n",
    "  seq_tensor = torch.stack(seq_tensor).float()\n",
    "  label_tensor = torch.stack(label_tensor).float()\n",
    "\n",
    "  original = seq_tensor.view(1,max_seq_len,23)\n",
    "  seq_tensor = seq_tensor.view(1,max_seq_len,23)\n",
    "  label_tensor = seq_tensor.view(1,max_seq_len,23)\n",
    "\n",
    "  seq_tensor = torch.transpose(seq_tensor,1,2).cuda()\n",
    "  pred,x = test(seq_tensor)\n",
    "  \n",
    "  pred = torch.transpose(pred,1,2)\n",
    "  seq_tensor = torch.transpose(seq_tensor,1,2)\n",
    "  break\n",
    "\n",
    "print(\"\\nOriginal\")\n",
    "ori_str = print_seq(original[0].view(1,original.size()[1], original.size()[2]), valid_elems, acids)\n",
    "\n",
    "print(\"\\nInput\")\n",
    "inp_str = print_seq(seq_tensor[0].view(1,seq_tensor.size()[1], seq_tensor.size()[2]), valid_elems, acids)\n",
    "\n",
    "print(\"\\nPrediction\")\n",
    "pred_str = print_seq(pred[0].view(1,pred.size()[1], pred.size()[2]), valid_elems, acids)\n",
    "\n",
    "print(\"\\Label\")\n",
    "label_str = print_seq(label_tensor[0].view(1,label_tensor.size()[1], label_tensor.size()[2]), valid_elems, acids)\n",
    "\n",
    "print\n",
    "acc = 0\n",
    "\n",
    "acc = np.sum([1 if pred_elem == original_elem else 0 for pred_elem, original_elem in zip(inp_str,pred_str)])/len(pred_str)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = []\n",
    "for i, seq in enumerate(SeqIO.parse(small_file, \"fasta\")):\n",
    "  if (len(seq.seq) > max_seq_len) or (('X' in seq.seq) or ('B' in seq.seq) or ('Z' in seq.seq) or ('J' in seq.seq)):\n",
    "    continue\n",
    "  seq_tensor = str(seq.seq).ljust(max_seq_len, '-')\n",
    "  seq_tensor = [dataset.acid_dict[seq_elem] for seq_elem in seq_tensor]\n",
    "  seq_tensor = torch.stack(seq_tensor).float()\n",
    "  seq_tensor = seq_tensor.view(1,max_seq_len,23)\n",
    "  seq_tensor = torch.transpose(seq_tensor,1,2).cuda()\n",
    "  _, low_dim = test(seq_tensor)\n",
    "  low_dim = torch.squeeze(torch.squeeze(low_dim,0),0)\n",
    "  data_points.append((low_dim.cpu()).detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_points = np.array(data_points)\n",
    "x = data_points[:,0]\n",
    "y = data_points[:,1]\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()\n",
    "#z = low_dim_points[:,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
