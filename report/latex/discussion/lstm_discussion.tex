\subsubsection{Layers vs no layers}
In this experiment we tried to figure out whether or not stacking an LSTM would help increase the performance of the network. The idea was that adding this extra non-linearity would make it able to find better patterns in the structures of these proteins, and thereby get better results. The results show that simply stacking them without anything else does not actually provide any kind of improvement, quite the opposite in fact. The resulting Spearman's rho values of $0.428$ for the single layer and $0.400$ for the double layered models attest to this. One possible explanation for this could be that the model begins overfitting on the data when stacked in this way. It is interesting to note that while the 1-layer model gets a better Spearman correlation score, its next token prediction accuracy is slightly worse. Considering the fact that a 2-layer LSTM takes roughly twice as long to train as a 1-layer LSTM, stacking them like this does not seem like a good way to increase the performance of an LSTM for this task.

\subsubsection{Dropout vs. no dropout}
In this experiment we tested whether adding dropout between the layers of a stacked LSTM would help increase the robustness of the network. Now, considering the results from the previous experiment, we weren't expecting a significant improvement over the baseline single layer model. The results were very interesting however; the next token prediction accuracy was basically the same, with only a $0.01\%$ difference on the test set. Meanwhile, the Spearman correlation score improved quite significantly, scoring $0.518$. This is may be because the dropout forced the network to learn a better general representation of the structure, rather than learning a few signs that may not always be there. This model takes a slightly shorter time to train than the network without dropout. Because of this, adding dropout between layers of a stacked LSTM seems to be a great way to increase both the performance and robustness of the network.

\subsubsection{Feature size}
It is to be expected that reducing the amount of features a model can use will limit how much it can learn. For this experiment we decided to see whether the increased amount of data a smaller model can be trained on due to the increased speed would make up for that lack of features. For reference, the smallest model at $256$ hidden features trained for $60$ epochs, while the largest at $1024$ hidden features only trained for $10$. The model with $512$ features trained for $30$ epochs. We expected that there would be some middle ground between speed and the number of features, but actually saw that increasing the features was a boost to performance in all the cases we tested. Since we were limited by hardware, going over $1024$ features was not possible. An interesting note is that there is a very large difference between how the predictions
\subsubsection{Learning rate}

\subsubsection{Minimum loss model vs last model}