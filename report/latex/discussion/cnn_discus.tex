In the process of constructing the CNN, we had to do a lot of hyperparameter tuning before coming up with the final model. In the pursuit of getting the best performing model, we started with a sanity check. We did this by only introducing the bottleneck to the channel dimension, which led to a 100\% accuracy in reconstruction. Once this was done, we started adding a bottleneck to the feature size as well. At this point, we started with tuning some of the hyperparameters: the number of hidden layers, and the sizes of each hidden layer. \\

\noindent
In theory, increasing the number of hidden layers will increase the models potential of solving more complex tasks, corresponding to the abstraction of the dataset.\\

\noindent
We experienced that when increasing the number of hidden layers, the loss would at some point start increasing. Yet, turning the learning rate down didn't solve the problem. Thus, the problem could be that the probability distribution in the cross-entropy loss function converged to all zeros. This could be solved by adding some epsilon in the calculation of the probability distribution. We ended up settling with the layers explained in the CNN architecture section. Thus, avoiding the increasing loss. \\

\noindent
We started to tune the sizes of each hidden layer. Meaning the way the dimensionality changed over each specific hidden layer. This was simply a trial and error task, leading to what yielded the best results.

\noindent
We only had one experiment with CNN, consisting of changing both the latent dimension and the channel size, which does not cover the full potential of a convolutional network. When checking how much impact the latent dimension had on the results, we should have made experiments with only reducing and increasing either the channel size or the feature size. Doing so would have made it easier to tell how each parameter effects the end result. \\

\noindent
During the trial and error phase, we failed to document how each of the variable tunings affected the model. This should have been documented since it would have made it easier to understand our path towards the final model.

\noindent
The CNN ended up performing poorly compared to the LSTM. Our CNN was a relatively simple model, trying to solve a very complex problem. Thus, we can't rule out whether or not a CNN can be used to solve this specific problem. We chose to use a bottleneck, which should force the model to learn how to encode some structural information about the proteins. This might not be the optimal way of learning useful insights.