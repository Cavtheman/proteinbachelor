The optimizer we are using in our models is the Adam (adaptive moment estimation) optimizer. The Adam optimizer is based on stochastic gradient descent\cite{adam}, therefor Adam also has a random probability of picking a gradient in the sample data. As well as SGD, Adam is well suited for objective functions, which require some scalar parameter maximization or minimization. \\

\noindent
Adam also uses something called gradient descent with momentum. Momentum is a method, which computes an exponentially weighted average of previously calculated gradients, and uses this as gradient in the gradient descent step.

\noindent
when talking about optimizers in neural networks, "learning rate" is a keyword in this context. The learning rate is a parameter, which defines the rate of change given a gradient. This means that the greater the learning rate is, the greater the change of some specific parameter will become; in the context of finding the local minima.\\

\noindent
Adam differentiates from SGD, the way it dynamically can adjust the magnitude of how it updates the scalar parameters based on the momentum of the gradient. this means that Adam is capable of computing adaptive learning rates for different parameters\cite{adam}. Thus, for each iteration of updates in the neural network, the learning rate will be updated as well. To simplify what this means; if a gradient with respect to some parameter, has not changed for a few iterations, the learning rate for that specific gradient will be decreased.