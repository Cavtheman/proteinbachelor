% RNN vs CNN
% Protein Representation
%
Proteins encode many different significant and more or less useful properties in the organisms they exist in. Proteins consist of long chains of up to several hundred amino acids, with some of these being more important for the proteins properties than others. Now, how a protein interacts with other proteins is primarily based on their $3$D shape, which is generally called their tertiary structure. A Proteins primary structure consists of the amino acid chain, while their secondary structure is the shape that these chains form. (eg. helices or sheets). The tertiary structure would then describe the way that these are combined and folded in $3$D space. Because their interactions with other proteins are primarily affected by their tertiary structure, finding this is very important for understanding a protein's function. \\

\noindent
Finding a protein's tertiary structure has historically been a very slow and expensive process however, generally requiring significant amounts of lab work and time. On the contrary, finding their primary structures (amino acid sequences) is relatively very easy, and the amount of data available in this form is actually increasing exponentially.\cite{seqcount} This means that there is a significantly larger amount of data available with the primary structure of proteins. Thus, finding a meaningful representation that can be used to say something about other properties of a protein using only the primary structure is an active area of research. There are many parallels between tertiary structure prediction and natural language processing (NLP) in that the data consists of sequences of unknown length containing many long-range dependencies between subsequences with high importance.\\

\noindent
Recurrent Neural Networks (RNNs) in various forms have been go-to architectures for most modelling tasks regarding these types of sequences for a long time now and with good reason. They provide good results and can, given proper architectures, remember many of the dependencies between subsequences and achieve state-of-the-art performance on many protein representation and prediction tasks. However, a recent paper has shown that in many cases, Convolutional Neural Networks (CNNs) can achieve as good, if not better results when applied properly.\cite{intro} We will in this paper reproduce an RNN that can learn some representation from these raw sequences by training it for next-token prediction.\cite{unirep} The idea here is that if the model can accurately predict what the next amino acid is in these long chains, it is because it has learned something inherent in the structure of this protein.\\

\noindent
We will then compare these results to a CNN that is trained to predict each token in the input sequence. This is an easy task, except the input will be changed to something incorrect. In this case, the idea is that the model will end up having to learn what each element is supposed to be from the context around it. We do this by encoding the primary structure of the protein into an arbitrarily small representation, and then decoding this representation to output the same primary structure. Having learned how to do this well must mean that it has also learned something inherent about the structure of these proteins, and thus the encoding of the protein must have some meaningful information about its structure.\\

\noindent
To evaluate the very different results of these models, we will be training a simple model on the representation that our more complex models learn, against a dataset that describes how stable a protein is. Since this stability is entirely based on the proteins structure, having a good representation of this must mean that it is relatively easy to predict the proteins stability.