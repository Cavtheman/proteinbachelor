% RNN vs CNN
% Protein Representation
%
Finding a protein's secondary structure (eg. alpha helices and beta sheets) has historically been a very slow and expensive process, generally requiring significant lab work. On the contrary, finding their primary structures (amino acid sequences) is relatively very easy. This means that there is a significantly larger amount of data available with the primary structure of proteins. Because of this, trying to predict the secondary structure of proteins using only their primary structure is an active area of research. There are many parallels between protein structure prediction and natural language processing (NLP) in that the data consists of sequences of unknown length containing many long range dependencies between data points with high importance. 

Recurrent Neural Networks (RNNs) in various forms have been go-to architectures for most modelling tasks regarding these types of sequences for a long time now, and with good reason. They provide good results and can, given proper architectures, remember many of these dependencies and achieve state-of-the-art performance on many of these tasks. However, a recent paper has shown that in many cases, Convolutional Neural Networks (CNNs) can achieve as good, if not better results when applied properly.\cite{intro} We will in this paper reproduce an RNN that can learn some representation from these raw sequences by training it for next-token prediction.\cite{unirep} The idea here is that if the model can accurately predict what the next amino acid is in these long chains, it is because it has learned something inherent in the structure of this protein.

We will then compare these results to a CNN that is trained to predict each token in the input sequence. This is an easy task, except the input will be changed to something incorrect. In this case, the idea is that the model will end up having to learn what each element is supposed to be from the context around it. We do this by encoding the data from the protein down to an arbitrary size, and then using this encoding to decode it back. Having learned how to do this must mean that it has also learned something inherent about the structure of these proteins. 

To evaluate the very different results of these models, we will be training a simple linear regression model on the average of the RNNs hidden state over each sequence, and on the CNNs encoding of the protein. These will be evaluated against the stability dataset\cite{stability} as per the TAPE paper.\cite{tape}