Going into this project, our first goal was to reproduce the results of the Unirep paper\cite{unirep}. In most respects, we used the same architecture for our LSTM network. There are a few differences though. They used a $1900$ feature, single hidden layer mLSTM, whereas we used a standard LSTM with $600$ features and 2 layers with $0.5$ dropout. They also performed some slightly different preprocessing of their data, though this should not have a large effect on the model's performance; They trained on sequences that were up to $2000$ amino acids long, while we only trained on sequences with $500$. Due to hardware constraints, we could also only load a dataset with $78$k sequences, while they trained on $24$ million sequences for three weeks on four separate GPUs. Meanwhile, our model that has trained the longest only trained for $8$ hours, on a single MX250 laptop GPU.