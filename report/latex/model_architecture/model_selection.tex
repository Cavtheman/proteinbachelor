Choosing the right models, can in some cases be difficult. When training a neural network, there are a lot of hyperparameters that should be taken into consideration. in the pursuit of creating the most optimal neural network, it leads to a long row of hyperparameters which should be optimized. This includes the number of hidden layers, hidden neurons per each layer, learning rate, and regularization parameters. \\

\noindent
There is no method for choosing an optimal architecture. A good architecture depends on the task - what are the model trying to achieve. Neural networks come in different forms, and each can vary a lot from each other. Tuning these hyperparameters, by trying every single combination, trying to find the ones that yield the best result, is a too comprehensive task for most computers. \\

\noindent
in our case, we have experimented with two models, a simple bottleneck CNN, and an LSTM RNN where we are replicating the work done in unirep\cite{unirep}. During this process, we used our expertise in neural networks, in choosing the correct hyperparameters for the task.
