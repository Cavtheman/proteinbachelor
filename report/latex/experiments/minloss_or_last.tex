This is a slightly unconventional experiment, in that it came about as a response to the same problem that was described in the learning rate experiment. The training loss begins varying significantly as training goes on. For this experiment, we use the models from all the previous experiments. Each model in the previous experiments has two "versions". One version is the "last" trained model. Meaning it is not necessarily the model that scored best on the training and validation sets. But the result of the last optimization in the training. The second version is the model that performed best during training. These will be referred to as the "final" and "minloss" models respectively.\\

\noindent
The idea behind this experiment is that we wanted to see whether or not picking the minloss model would have any significant effect on performance. One could reasonably argue that when the difference between these models is so large, it could be because the minloss model is overfitting. Alternatively, it could be because the final model is underfitting.\\

\noindent
Based on the fact that the models do not achieve very high next token prediction accuracy, we hypothesize that using the minloss models will improve both this accuracy and spearman correlation scores. This is because we suspect that the models are already underfitting.