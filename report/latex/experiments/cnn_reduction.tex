Using the CNN we reduce the dimension of each sequence to some latent representation $z$. Using $z$ in combination with t-sne, the goal was to get a two dimensional representation of each sequence, showing that some seperation could be seen in between the structures. \\

\noindent
During the experimenting with this model, we worked with a learning rate of $lr=1e-4$ and a batch size of 32. Since we experienced that introducing a too steep bottle neck would yield worse results, we decided to reduce the channel size down to 4 dimensions. \\

\noindent
The way our experiements vary from each other is how much we reduce the feature-dimension. Our two experiements work with the latent feature-dimensions 50 and 100. thus the $z$ will either have a size of $4$x$50$ or $4$x$100$. the latent representation will then get flattened and used in t-sne. \\
