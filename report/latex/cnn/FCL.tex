When taking convolutional layers, ReLU layers and pooling layers, stacking them, in way so the output of the first operation becomes the input of the next operation in the stack. When these operations get stacked like this, fully connected layers are often introduced at the end of the last convolution. These fully connected layers are the same as the hidden layers in a feed-forward neural network. In a CNN you can have non-convolutional layers, though at least one is required for it to be a CNN. CNNs can have multiple of these layers. Normally the last hidden layer before the output layer is a convolutional layer since the output of this only shows patterns that a fully connected layer can pick up on, to come up with a specific prediction of a class etc.
