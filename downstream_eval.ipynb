{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils import data\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Our code\n",
    "from data_generator import Dataset\n",
    "from lstm import LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_file = \"stab_data/stability_train.json\"\n",
    "valid_file = \"stab_data/stability_valid.json\"\n",
    "test_file = \"stab_data/stability_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class stab_dataset(Dataset):\n",
    "    def __init__(self, filename, max_seq_len, output_type=\"onehot\", acids=\"ACDEFGHIKLMNPQRSTVWY-\"):\n",
    "        elem_list = []\n",
    "        label_list = []\n",
    "        self.acids = acids\n",
    "        self.output_type = output_type\n",
    "        self.acid_dict, self.int_acid_dict = self.__gen_acid_dict__(acids)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.get_prot_class=True\n",
    "        # Loading the entire input file into memory\n",
    "        for i, elem in enumerate(json.load(open(filename))):\n",
    "            seq = elem[\"primary\"].upper()\n",
    "            if self.__is_legal_seq__(seq):\n",
    "                elem_list.append(seq)\n",
    "                label_list.append(elem[\"stability_score\"][0])\n",
    "        '''\n",
    "        for i, elem in enumerate(SeqIO.parse(filename, \"fasta\")):\n",
    "            if self.__is_legal_seq__(elem.seq.upper()):\n",
    "                elem_list.append(elem.seq.upper())\n",
    "                if get_prot_class:\n",
    "                    label_list.append(prot_class_re.search(elem.description).group(1))\n",
    "        '''\n",
    "        \n",
    "        self.data = elem_list\n",
    "        self.prot_labels = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class lin_reg(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(lin_reg, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.long()\n",
    "        return self.linear(x)\n",
    "\n",
    "# Use Cuda if available\n",
    "use_cuda = torch.cuda.is_available() and True\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "loaded_params = torch.load(\"best_lstm.pth\")\n",
    "\n",
    "max_seq_len = 500\n",
    "batch_size = 64\n",
    "hidden_dim = 512\n",
    "embed_size = 30\n",
    "hidden_layers = 1\n",
    "\n",
    "scope_file = \"scope_data_40.fasta\"\n",
    "acids = \"ACDEFGHIKLMNOPQRSTUVWY-\"\n",
    "\n",
    "stab_train = stab_dataset(train_file, 500, output_type=\"embed\", acids=acids)\n",
    "stab_valid = stab_dataset(valid_file, 500, output_type=\"embed\", acids=acids)\n",
    "stab_test = stab_dataset(test_file, 500, output_type=\"embed\", acids=acids)\n",
    "\n",
    "train_generator = data.DataLoader(stab_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valid_generator = data.DataLoader(stab_valid, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_generator = data.DataLoader(stab_test, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "#model = LSTM_model(len(acids), embed_size, hidden_dim, hidden_layers, max_seq_len, batch_size, processor, dropout=0).to(processor)\n",
    "model = LSTM_model(**loaded_params[\"args_dict\"]).to(processor)\n",
    "model.load_state_dict(loaded_params[\"state_dict\"])\n",
    "\n",
    "top_model = lin_reg(hidden_dim, 1).to(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = float(\"inf\")\n",
    "time_diff = 0\n",
    "no_improv = 0\n",
    "min_loss = float(\"inf\")\n",
    "val_loss = float(\"inf\")\n",
    "epochs = 20\n",
    "loss_function = nn.MSELoss() \n",
    "optimiser = torch.optim.SGD(top_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (batch, labels, valid_elems, scores) in enumerate(train_generator):\n",
    "\n",
    "        # Keeping track of stuff\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        est_time_left = str(time_diff*(min(batches, stab_train.__len__()/batch_size) - i) + (time_diff*min(batches, stab_train.__len__()/batch_size)) * (epochs - (epoch+1))).split(\".\")[0]\n",
    "        sys.stdout.write(\"\\rEpoch: {0}. Batch: {1}. Train: {2:.5f}. Valid: {5:.5f} Estimated time left: {3}. Best: {4} batches ago.\".format(epoch+1, i+1, min_loss, est_time_left, no_improv, val_loss))\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        #model.eval()\n",
    "        top_model.train()\n",
    "        scores.requires_grad_(True)\n",
    "        batch = batch.to(processor)\n",
    "        labels = labels.to(processor)\n",
    "        scores = scores.to(processor)\n",
    "        \n",
    "        \n",
    "        # Transposing from (batch x seq x feature_size) to (seq x batch x feature_size)\n",
    "        batch = batch.transpose(0,1)\n",
    "        \n",
    "        out, hidden = model(batch, valid_elems)\n",
    "        \n",
    "        reduced_hidden = torch.mean(hidden, dim=0)\n",
    "        \n",
    "        #pred = top_model(test).squeeze(1)\n",
    "        pred = top_model(reduced_hidden).squeeze(1)\n",
    "\n",
    "        #print(pred[:64])\n",
    "\n",
    "        loss = loss_function(pred, scores)\n",
    "        #print(\"Loss:\", loss)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        if loss.item() < min_loss:\n",
    "            torch.save(top_model.state_dict(), \"temp_best_down_model.pth\")\n",
    "            min_loss = loss.item()\n",
    "            no_improv = 0\n",
    "        else:\n",
    "            no_improv += 1\n",
    "        \n",
    "        if i%50 == 0:\n",
    "            with torch.no_grad():\n",
    "                top_model.eval()\n",
    "                val_loss = 0\n",
    "                for j, (batch, labels, valid_elems, scores) in enumerate(valid_generator):\n",
    "                    scores.requires_grad_(True)\n",
    "                    batch = batch.to(processor)\n",
    "                    labels = labels.to(processor)\n",
    "                    scores = scores.to(processor)\n",
    "                    batch = batch.transpose(0,1)\n",
    "        \n",
    "                    out, hidden = model(batch, valid_elems)\n",
    "        \n",
    "                    reduced_hidden = torch.mean(hidden, dim=0)\n",
    "        \n",
    "                    pred = top_model(reduced_hidden).squeeze(1)\n",
    "\n",
    "                    val_loss += loss_function(pred, scores).item()\n",
    "\n",
    "                    if j+1 >= 5:\n",
    "                        break\n",
    "                val_loss /= j + 1\n",
    "\n",
    "        # For tracking progress\n",
    "        end_time = datetime.now()\n",
    "        time_diff = end_time - start_time\n",
    "        \n",
    "        # Breaking when it's run through the given number of batches\n",
    "        if i+1 >= batches:\n",
    "            \n",
    "            break\n",
    "    torch.save(top_model.state_dict(), \"temp_down_model.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_preds = None\n",
    "load_top = False\n",
    "if load_top:\n",
    "    top_model.load_state_dict(torch.load(\"temp_down_model.pth\"))\n",
    "\n",
    "    \n",
    "for i, (batch, labels, valid_elems, scores) in enumerate(test_generator):\n",
    "    scores.requires_grad_(True)\n",
    "    batch = batch.to(processor)\n",
    "    labels = labels.to(processor)\n",
    "    scores = scores.to(processor)\n",
    "    batch = batch.transpose(0,1)\n",
    "    \n",
    "    out, hidden = model(batch, valid_elems)\n",
    "\n",
    "    reduced_hidden = torch.mean(hidden, dim=0)\n",
    "    \n",
    "    preds = top_model(reduced_hidden).squeeze(1)\n",
    "\n",
    "    if full_preds is None:\n",
    "        full_preds = preds.detach()\n",
    "        full_scores = scores.detach()\n",
    "    else:\n",
    "        full_preds = torch.cat((full_preds, preds.detach()))\n",
    "        full_scores = torch.cat((full_scores, scores.detach()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(full_preds.size())\n",
    "print(full_scores.size())\n",
    "print(full_preds[:64])\n",
    "print(full_scores[:64])\n",
    "spearman_vals = spearmanr(full_preds.cpu(), full_scores.cpu())\n",
    "print(spearman_vals)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12,8))\n",
    "ax.scatter(full_preds.cpu(), full_scores.cpu(), s=2)\n",
    "line = np.linspace(-0.5,2, 10)\n",
    "ax.plot(line, line, color=\"red\")\n",
    "ax.set_title(\"Spearman's Rho: {0:.3f}\".format(spearman_vals[0]))\n",
    "ax.set_xlabel(\"Stability score prediction\")\n",
    "ax.set_ylabel(\"True Stability score\")\n",
    "ax.axis(\"equal\")\n",
    "fig.savefig(\"spearman_correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "downstream_eval.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
