{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from Bio import SeqIO\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils import data\n",
    "#from data_generator import data_generator\n",
    "from data_generator import Dataset\n",
    "from lstm import LSTM_model\n",
    "from lstm import LSTMCell\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_alphabet(mod_val):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    index = mod_val % 26\n",
    "    return alphabet[index:] + alphabet[:index]\n",
    "\n",
    "test = [gen_alphabet(i) for i in range(32)]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class alpha_set(data.Dataset):\n",
    "    def __gen_acid_dict__(self, acids):\n",
    "        acid_dict = {}\n",
    "        for i, elem in enumerate(acids):\n",
    "            temp = torch.zeros(len(acids))\n",
    "            temp[i] = 1\n",
    "            acid_dict[elem] = temp\n",
    "        return acid_dict\n",
    "    \n",
    "    def __init__(self, acids, length, num_seqs):\n",
    "        self.max_seq_len = length\n",
    "        self.acids = acids\n",
    "        self.acid_dict = self.__gen_acid_dict__(acids)\n",
    "        self.data = [gen_alphabet(i) for i in range(num_seqs)]\n",
    "\n",
    "    def __prepare_seq__(self, seq):\n",
    "        valid_elems = min(len(seq)+1, self.max_seq_len)\n",
    "        seq = str(seq).ljust(self.max_seq_len+1, '-')\n",
    "        temp_seq = [self.acid_dict[x] for x in seq]\n",
    "        tensor_seq = torch.stack(temp_seq[:-1]).float()\n",
    "        #valid_elems = torch.Tensor([elem != '-' for elem in seq[:-1]])\n",
    "\n",
    "        # Labels consisting of the raw tensor\n",
    "        # labels_seq = torch.stack(temp_seq[1:]).long()\n",
    "\n",
    "        # Label consisting of last element\n",
    "        # labels_seq = temp_seq[-1].long()\n",
    "\n",
    "        # Labels consisting of the index of correct class\n",
    "        labels_seq = torch.argmax(torch.stack(temp_seq[1:]), dim=1).long()\n",
    "\n",
    "        #print(labels_seq.size())\n",
    "        #print(tensor_seq.size())\n",
    "        #labels_seq = torch.transpose(labels_seq, 0, 1)\n",
    "        #tensor_seq = torch.transpose(tensor_seq, 0, 1)\n",
    "        #print(\"Seq shape:\", tensor_seq[1:].size())\n",
    "        return tensor_seq, labels_seq, valid_elems\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.__prepare_seq__(self.data[index])\n",
    "\n",
    "# Use Cuda if available\n",
    "use_cuda = torch.cuda.is_available() and True\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "batch_size = 1\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz-\"\n",
    "alpha_dataset = alpha_set(alphabet, 64, 10000)\n",
    "alpha_generator = data.DataLoader(alpha_dataset, batch_size=batch_size, shuffle=True)\n",
    "loss_function = nn.CrossEntropyLoss(reduction=\"sum\").to(processor)\n",
    "lstm = LSTM_model(len(alphabet), 100, 1, 27).to(processor)\n",
    "optimiser = optim.SGD(lstm.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "\n",
    "for i, (seq, label, valid) in enumerate(alpha_generator):\n",
    "    seq = seq.to(processor)\n",
    "    label = label.to(processor)\n",
    "    valid = valid.to(processor)\n",
    "    \n",
    "    seq = seq.transpose(0,1)\n",
    "    #label = label.transpose(0,1)\n",
    "    label = label.squeeze(0)\n",
    "    if i == 0:\n",
    "        print(\"Input:\\t\", seq.size())\n",
    "        print(\"Labels:\\t\", label.size())\n",
    "\n",
    "    \n",
    "    lstm.zero_grad()\n",
    "\n",
    "    out, hidden = lstm(seq)\n",
    "    out = out.squeeze(1)\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"Output:\\t\", out.size())\n",
    "        print(\"Hidden:\\t\", hidden.size())\n",
    "    #out = out.transpose(1, 2)\n",
    "    \n",
    "    loss = loss_function(out, label)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (seq, label, valid) in enumerate(alpha_generator):\n",
    "        seq = seq.to(processor)\n",
    "        label = label.to(processor)\n",
    "        valid = valid.to(processor)\n",
    "        \n",
    "        seq = seq.transpose(0,1)\n",
    "        #label = label.transpose(0,1)\n",
    "        label = label.squeeze(0)\n",
    "        \n",
    "        out, hidden = lstm(seq)\n",
    "\n",
    "        print(out.size())\n",
    "        #print(seq)\n",
    "        print(\"Predictions:\", torch.argmax(out, dim=2).transpose(0,1))\n",
    "        print(\"Labels:\", label)\n",
    "        #print(torch.argmax)\n",
    "\n",
    "        if i > 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alternatively, we can do the entire sequence all at once.\n",
    "# the first value returned by LSTM is all of the hidden states throughout\n",
    "# the sequence. the second is just the most recent hidden state\n",
    "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
    "# by passing it as an argument  to the lstm at a later time\n",
    "# Add the extra 2nd dimension\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]\n",
    "lstm = nn.LSTM(3,3)\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "\n",
    "print(inputs.size())\n",
    "\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "\n",
    "print(hidden[0].size(), hidden[1].size())\n",
    "\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out.size())\n",
    "print(out, \"\\n\")\n",
    "print(hidden)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "lstm_testing.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
