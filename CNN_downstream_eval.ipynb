{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from torch.utils import data\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Our code\n",
    "from data_generator import Dataset\n",
    "from lstm import LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"stab_data/stability_train.json\"\n",
    "valid_file = \"stab_data/stability_valid.json\"\n",
    "test_file = \"stab_data/stability_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stab_dataset(Dataset):\n",
    "    def __init__(self, filename, max_seq_len, output_type=\"onehot\", acids=\"ACDEFGHIKLMNPQRSTVWY-\"):\n",
    "        elem_list = []\n",
    "        label_list = []\n",
    "        self.acids = acids\n",
    "        self.output_type = output_type\n",
    "        self.acid_dict, self.int_acid_dict = self.__gen_acid_dict__(acids)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.get_prot_class=True\n",
    "        # Loading the entire input file into memory\n",
    "        for i, elem in enumerate(json.load(open(filename))):\n",
    "            seq = elem[\"primary\"].upper()\n",
    "            if self.__is_legal_seq__(seq):\n",
    "                elem_list.append(seq)\n",
    "                label_list.append(elem[\"stability_score\"][0])\n",
    "        '''\n",
    "        for i, elem in enumerate(SeqIO.parse(filename, \"fasta\")):\n",
    "            if self.__is_legal_seq__(elem.seq.upper()):\n",
    "                elem_list.append(elem.seq.upper())\n",
    "                if get_prot_class:\n",
    "                    label_list.append(prot_class_re.search(elem.description).group(1))\n",
    "        '''\n",
    "        \n",
    "        self.data = elem_list\n",
    "        self.prot_labels = label_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class lin_reg(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(lin_reg, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = x.long()\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() and True\n",
    "print(\"Using GPU:\", use_cuda)\n",
    "processor = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 500\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "scope_file = \"scope_data_40.fasta\"\n",
    "acids = \"ACDEFGHIKLMNOPQRSTUVWY-\"\n",
    "\n",
    "stab_train = stab_dataset(train_file, 500, output_type=\"embed\", acids=acids)\n",
    "stab_valid = stab_dataset(valid_file, 500, output_type=\"embed\", acids=acids)\n",
    "stab_test = stab_dataset(test_file, 500, output_type=\"embed\", acids=acids)\n",
    "\n",
    "train_generator = data.DataLoader(stab_train, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "valid_generator = data.DataLoader(stab_valid, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_generator = data.DataLoader(stab_test, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "#model = LSTM_model(len(acids), embed_size, hidden_dim, hidden_layers, max_seq_len, batch_size, processor, dropout=0).to(processor)\n",
    "model = LSTM_model(**loaded_params[\"args_dict\"]).to(processor)\n",
    "model.load_state_dict(loaded_params[\"state_dict\"])\n",
    "\n",
    "top_model = lin_reg(hidden_dim, 1).to(processor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
